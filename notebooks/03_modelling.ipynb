{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139361d4",
   "metadata": {},
   "source": [
    "# Xây dựng mô hình\n",
    "Chúng ta sẽ xây dựng 3 mô hình cơ bản "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9864ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f6fce",
   "metadata": {},
   "source": [
    "## I. Thuật toán hồi quy Logistic (Logistic Regression) sử dụng phương pháp Gradient Descent.\n",
    "\n",
    "Đây là một thuật toán học máy có giám sát (supervised learning) được sử dụng cho bài toán **phân loại nhị phân** (binary classification) - ví dụ: phân loại 0 hoặc 1, đúng hoặc sai.\n",
    "\n",
    "---\n",
    "\n",
    "Việc huấn luyện mô hình để tìm ra bộ tham số trọng số ($w$) và hệ số chệch ($b$) tốt nhất, giúp phân tách dữ liệu thành 2 lớp. Quá trình này diễn ra như sau:\n",
    "\n",
    "### Bước 1: Khởi tạo (Initialization)\n",
    "\n",
    "  * **Mục đích:** Tạo điểm bắt đầu cho mô hình.\n",
    "  * **Trong code:** `self.w = np.zeros(n)` và `self.b = 0`.\n",
    "  * **Giải thích:** Ban đầu, mô hình chưa học được gì nên ta đặt trọng số $w$ là một vector số 0 và $b$ là 0.\n",
    "\n",
    "### Bước 2: Quá trình Huấn luyện (Training Loop - `fit`)\n",
    "\n",
    "Vòng lặp chạy `epochs` lần, mỗi lần lặp gồm 3 pha chính:\n",
    "\n",
    "**Pha 1: Lan truyền xuôi (Forward Propagation - Prediction)**\n",
    "\n",
    "1.  **Tính tổ hợp tuyến tính ($z$):**\n",
    "      * Công thức: $z = w \\cdot X + b$.\n",
    "      * Đây là phương trình đường thẳng (hoặc siêu phẳng) phân chia không gian dữ liệu.\n",
    "2.  **Hàm kích hoạt Sigmoid:**\n",
    "      * Công thức: $\\hat{y} = \\frac{1}{1 + e^{-z}}$.\n",
    "      * **Trong code:** `np.clip(z, -250, 250)` được dùng trước khi đưa vào hàm mũ `np.exp`. Điều này để tránh lỗi tràn số (overflow) khi $z$ quá lớn hoặc quá nhỏ, giữ cho việc tính toán ổn định.\n",
    "      * Kết quả $\\hat{y}$ (biến `y_pred`) là xác suất (từ 0 đến 1) điểm dữ liệu thuộc về lớp 1.\n",
    "\n",
    "**Pha 2: Tính Gradient (Gradient Calculation)**\n",
    "\n",
    "  * Mục đích: Tính toán đạo hàm của hàm mất mát (Loss function) để biết mô hình đang sai lệch bao nhiêu và cần điều chỉnh theo hướng nào.\n",
    "  * **Công thức đạo hàm:**\n",
    "      * Đối với $w$: $dw = \\frac{1}{m} X^T (\\hat{y} - y)$\n",
    "      * Đối với $b$: $db = \\frac{1}{m} \\sum (\\hat{y} - y)$\n",
    "  * Trong code, phép tính này được **vector hóa** (vectorized) bằng `np.dot`, giúp tính toán nhanh hơn nhiều so với dùng vòng lặp `for` từng điểm dữ liệu.\n",
    "\n",
    "**Pha 3: Cập nhật tham số (Update Parameters)**\n",
    "\n",
    "  * Mục đích: Điều chỉnh $w$ và $b$ ngược hướng với Gradient để giảm thiểu sai số.\n",
    "  * Công thức:\n",
    "      * $w_{mới} = w_{cũ} - \\alpha \\times dw$\n",
    "      * $b_{mới} = b_{cũ} - \\alpha \\times db$\n",
    "  * Trong đó `lr` ($\\alpha$) là tốc độ học (learning rate).\n",
    "\n",
    "### Bước 3: Dự đoán (Prediction)\n",
    "\n",
    "  * Sau khi huấn luyện xong, dùng $w$ và $b$ đã tối ưu để tính xác suất cho dữ liệu mới.\n",
    "  * **Ngưỡng (Threshold):** Nếu xác suất \\> 0.5, kết luận là lớp 1. Ngược lại là lớp 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c49cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def fit(self, X, y, lr=0.01, epochs=1000):\n",
    "        m, n = X.shape\n",
    "        self.w = np.zeros(n)\n",
    "        self.b = 0\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # 1. Prediction (Vectorized)\n",
    "            z = np.dot(X, self.w) + self.b\n",
    "            y_pred = 1 / (1 + np.exp(-np.clip(z, -250, 250))) # Clip để tránh overflow\n",
    "            \n",
    "            # 2. Gradient Calculation (Vectorized)\n",
    "            dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/m) * np.sum(y_pred - y)\n",
    "            \n",
    "            # 3. Update\n",
    "            self.w -= lr * dw\n",
    "            self.b -= lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        y_pred = 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "        return np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101687dd",
   "metadata": {},
   "source": [
    "## II. Thuật toán K-Nearest Neighbors\n",
    "\n",
    "Đây là một thuật toán học máy đơn giản, thuộc loại **Học lười (Lazy Learning)** và **Phi tham số (Non-parametric)**. Đoạn code trên thực hiện bài toán **Phân loại (Classification)**.\n",
    "\n",
    "---\n",
    "\n",
    "Khác với Logistic Regression, KNN không \"học\" ra một bộ trọng số ($w, b$) cố định. Thay vào đó, nó ghi nhớ toàn bộ dữ liệu huấn luyện. Khi có dữ liệu mới cần dự đoán, nó sẽ tìm những điểm dữ liệu cũ giống (gần) điểm mới nhất để đưa ra quyết định.\n",
    "\n",
    "### Bước 1: Ghi nhớ dữ liệu (Training/Fit)\n",
    "\n",
    "  * **Trong code:** Hàm `fit` chỉ đơn giản gán `self.X_train = X` và `self.y_train = y`.\n",
    "  * **Giải thích:** Đây là đặc trưng của \"Học lười\". Thuật toán không tính toán gì cả cho đến khi cần dự đoán.\n",
    "\n",
    "### Bước 2: Dự đoán (Prediction)\n",
    "\n",
    "Khi nhận đầu vào $X$ mới, thuật toán thực hiện 3 công việc chính:\n",
    "\n",
    "**1. Tính khoảng cách (Vectorized Distance Calculation)**\n",
    "Mục tiêu là tính khoảng cách Euclidean giữa điểm mới và *tất cả* các điểm trong tập train.\n",
    "Công thức khoảng cách Euclidean chuẩn là: $d(x, y) = \\sqrt{\\sum (x_i - y_i)^2}$.\n",
    "Tuy nhiên, để tính nhanh trên ma trận lớn (vector hóa), đoạn code sử dụng hằng đẳng thức đáng nhớ:\n",
    "$$(A - B)^2 = A^2 + B^2 - 2AB$$\n",
    "\n",
    "  * **`X_sq` ($A^2$):** Bình phương và tổng các đặc trưng của dữ liệu cần dự đoán.\n",
    "  * **`Train_sq` ($B^2$):** Bình phương và tổng các đặc trưng của dữ liệu huấn luyện.\n",
    "  * **`dot_product` ($AB$):** Tích vô hướng giữa dữ liệu mới và dữ liệu huấn luyện.\n",
    "  * **`dists`:** Áp dụng công thức $\\sqrt{A^2 + B^2 - 2AB}$ để ra ma trận khoảng cách.\n",
    "\n",
    "**2. Tìm K láng giềng gần nhất (Finding Neighbors)**\n",
    "\n",
    "  * **`np.argsort`:** Sắp xếp các khoảng cách từ nhỏ đến lớn và lấy chỉ số (index) của chúng.\n",
    "  * **`[:, :self.k]`:** Chỉ giữ lại $k$ điểm có khoảng cách nhỏ nhất (gần nhất).\n",
    "  * **`k_labels`:** Từ các chỉ số tìm được, trích xuất nhãn ($y$) tương ứng của chúng.\n",
    "\n",
    "**3. Bầu chọn đa số (Majority Voting)**\n",
    "\n",
    "  * **Logic:** Xem trong $k$ láng giềng đó, lớp nào chiếm đa số (ví dụ: 3 điểm là lớp 1, 2 điểm là lớp 0 =\\> Kết quả là lớp 1).\n",
    "  * **Trong code:** Dùng `np.bincount` để đếm số lần xuất hiện của từng nhãn và `argmax` để lấy ra nhãn có số phiếu cao nhất.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9306610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def fit(self, X, y, k=5):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.k = k\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_sq = np.sum(X**2, axis=1, keepdims=True)\n",
    "        Train_sq = np.sum(self.X_train**2, axis=1)\n",
    "        dot_product = np.dot(X, self.X_train.T)\n",
    "        \n",
    "        dists = np.sqrt(X_sq + Train_sq - 2 * dot_product)\n",
    "        k_idx = np.argsort(dists, axis=1)[:, :self.k]\n",
    "        \n",
    "        k_labels = self.y_train[k_idx]\n",
    "        \n",
    "        return np.apply_along_axis(lambda x: np.bincount(x, minlength=2).argmax(), axis=1, arr=k_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7afb",
   "metadata": {},
   "source": [
    "## III. Thuật toán Gaussian Naive Bayes\n",
    "\n",
    "Đây là thuật toán phân loại dựa trên **Định lý Bayes**, với giả định \"ngây thơ\" (naive) rằng các đặc trưng (features) là độc lập với nhau và tuân theo phân phối chuẩn (Gaussian/Normal distribution).\n",
    "\n",
    "Mục tiêu cốt lõi: Tính xác suất một điểm dữ liệu thuộc về một lớp cụ thể và chọn lớp có xác suất cao nhất.\n",
    "Công thức: $P(Class | Data) \\propto P(Data | Class) \\times P(Class)$\n",
    "\n",
    "---\n",
    "\n",
    "Trong đoạn code này, thay vì dùng vòng lặp, ta sử dụng kỹ thuật **Broadcasting** của NumPy để tính toán song song trên ma trận 3 chiều (Samples x Classes x Features).\n",
    "\n",
    "### Bước 1: Huấn luyện (Fit - Thống kê dữ liệu)\n",
    "\n",
    "Mô hình \"học\" bằng cách tính toán các chỉ số thống kê cho từng lớp:\n",
    "\n",
    "1.  **Tách lớp (`mask`):** Tạo một ma trận mặt nạ để biết dòng dữ liệu nào thuộc lớp nào.\n",
    "2.  **Xác suất tiên nghiệm (`self.priors`):** Tỉ lệ xuất hiện của mỗi lớp trong tập huấn luyện ($P(Class)$).\n",
    "3.  **Trung bình (`self.mean`):** Giá trị trung bình của từng đặc trưng đối với từng lớp ($\\mu$).\n",
    "4.  **Phương sai (`self.var`):** Độ phân tán của dữ liệu quanh giá trị trung bình ($\\sigma^2$).\n",
    "      * *Lưu ý:* Code cộng thêm `1e-9` vào phương sai để làm mượt (smoothing), tránh lỗi chia cho 0 khi tính toán sau này.\n",
    "\n",
    "### Bước 2: Dự đoán (Predict - Tính toán xác suất)\n",
    "\n",
    "Thay vì tính xác suất trực tiếp (có thể ra số rất nhỏ gây lỗi underflow), thuật toán tính **Log-likelihood** (Logarit của hàm hợp lý).\n",
    "\n",
    "Công thức Gaussian Log-Likelihood cho một mẫu $x$:\n",
    "$$\\log P(x|c) = -\\frac{1}{2} \\sum \\left( \\log(2\\pi\\sigma_c^2) + \\frac{(x - \\mu_c)^2}{\\sigma_c^2} \\right)$$\n",
    "\n",
    "  * **`X_exp`:** Mở rộng chiều dữ liệu để khớp với kích thước của trung bình và phương sai các lớp.\n",
    "  * **`log_pdf`:** Tính toán độ \"khớp\" của dữ liệu mới với phân phối Gauss của từng lớp.\n",
    "      * Phần 1: $-0.5 \\sum \\log(2\\pi\\sigma^2)$ (Phần mẫu số của công thức Gauss).\n",
    "      * Phần 2: $-0.5 \\sum \\frac{(x - \\mu)^2}{\\sigma^2}$ (Phần số mũ, đo khoảng cách Mahalanobis bình phương).\n",
    "  * **`log_post`:** Cộng log xác suất tiên nghiệm (`priors`) vào kết quả. Đây chính là bước áp dụng định lý Bayes trong không gian Log.\n",
    "  * **`argmax`:** Chọn lớp có giá trị Log-posterior lớn nhất.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4456e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussianNB:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        m = X.shape[0]\n",
    "        mask = (y[:, None] == self.classes[None, :]).astype(float)\n",
    "\n",
    "        \n",
    "        class_count = mask.sum(axis=0)\n",
    "        self.priors = class_count / m\n",
    "        self.mean = (mask.T @ X) / class_count[:, None]\n",
    "\n",
    "        diff = X[:, None, :] - self.mean[None, :, :]    \n",
    "        self.var = (mask[:, :, None] * diff**2).sum(axis=0) / class_count[:, None]\n",
    "        self.var += 1e-9  \n",
    "\n",
    "    def predict(self, X):\n",
    "        X_exp = X[:, None, :]  \n",
    "\n",
    "        # Log likelihood\n",
    "        log_pdf = (\n",
    "            -0.5 * np.sum(np.log(2 * np.pi * self.var), axis=1)              # (k,)\n",
    "            -0.5 * np.sum(((X_exp - self.mean)**2) / self.var, axis=2)       # (m, k)\n",
    "        )  # final shape: (m, k)\n",
    "\n",
    "        log_post = log_pdf + np.log(self.priors)[None, :]  # shape (m, k)\n",
    "        return self.classes[np.argmax(log_post, axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c922f45",
   "metadata": {},
   "source": [
    "## IV. Xây dựng hàm `get_metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f47f5",
   "metadata": {},
   "source": [
    "Mục tiêu của ta là lấy các chỉ số đánh giá phân loại (`Classification Metrics`) dựa trên Ma trận nhầm lẫn (`Confusion Matrix`). Hàm này tính toán 3 chỉ số quan trọng nhất để đánh giá hiệu suất của một mô hình phân loại nhị phân (Binary Classification): **Precision (Độ chính xác), Recall (Độ nhạy)** và **F1-Score**.\n",
    "\n",
    "### Bước 1: Tính các thành phần cơ bản (Confusion Matrix)\n",
    "\n",
    "Code sử dụng các phép toán so sánh Boolean (`&`) và tính tổng (`np.sum`) để đếm:\n",
    "\n",
    "1.  **TP (True Positive - Dương tính thật):**\n",
    "      * Thực tế là 1, Mô hình đoán là 1.\n",
    "      * *Ý nghĩa:* Mô hình dự báo đúng lớp tích cực.\n",
    "2.  **FP (False Positive - Dương tính giả):**\n",
    "      * Thực tế là 0, Mô hình đoán là 1.\n",
    "      * *Ý nghĩa:* \"Báo động giả\" (Ví dụ: Người khỏe mạnh nhưng máy báo bị bệnh).\n",
    "3.  **FN (False Negative - Âm tính giả):**\n",
    "      * Thực tế là 1, Mô hình đoán là 0.\n",
    "      * *Ý nghĩa:* \"Bỏ sót\" (Ví dụ: Người bị bệnh nhưng máy báo khỏe mạnh).\n",
    "\n",
    "### Bước 2: Tính các chỉ số dẫn xuất\n",
    "\n",
    "1.  **Precision (Độ chính xác của dự báo dương):**\n",
    "\n",
    "      * Công thức: $P = \\frac{TP}{TP + FP}$\n",
    "      * *Logic:* Trong tất cả các trường hợp mô hình **phán là đúng**, có bao nhiêu phần trăm là **đúng thật**? \n",
    "\n",
    "2.  **Recall (Độ phủ / Độ nhạy):**\n",
    "\n",
    "      * Công thức: $R = \\frac{TP}{TP + FN}$\n",
    "      * *Logic:* Trong tất cả các trường hợp **thực tế là đúng**, mô hình **tìm ra được** bao nhiêu phần trăm? \n",
    "\n",
    "3.  **F1-Score:**\n",
    "\n",
    "      * Công thức: $F1 = 2 \\times \\frac{P \\times R}{P + R}$\n",
    "      * *Logic:* Là trung bình điều hòa (Harmonic Mean) của Precision và Recall. Chỉ số này đại diện cho sự cân bằng giữa độ chính xác và độ phủ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a2eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) else 0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return f1, recall, precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2eb02f",
   "metadata": {},
   "source": [
    "## V. MỘt số hàm hỗ trợ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542101b6",
   "metadata": {},
   "source": [
    "###  Hàm tải dữ liệu (`load_csv_numpy`)\n",
    "Dùng để tải file đã được xử lí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240c212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_numpy(path, skip_header=True, delimiter=\",\"):\n",
    "    return np.genfromtxt(path, delimiter=delimiter, skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6754f48",
   "metadata": {},
   "source": [
    "### Hàm chia dữ liệu kiểm thử (`k_fold_split`)\n",
    "\n",
    "* **Cách xử lý (Logic):**\n",
    "    1.  **Permutation:** Tạo ra một danh sách chỉ số (index) từ $0$ đến $N$ và xáo trộn ngẫu nhiên (Shuffle).\n",
    "    2.  **Slicing:** Cắt danh sách chỉ số thành $k$ phần bằng nhau.\n",
    "    3.  **Rotation (Xoay vòng):** Thực hiện vòng lặp $k$ lần:\n",
    "        * Chọn phần thứ $i$ làm **Test**.\n",
    "        * Gom tất cả các phần còn lại làm **Train**.\n",
    "    4.  **Return:** Trả về danh sách các cặp index `(Train_idx, Test_idx)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f80d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(X, y, test_ratio=0.2):\n",
    "    m = X.shape[0]\n",
    "    idx = np.random.permutation(m)\n",
    "    \n",
    "    test_size = int(m * test_ratio)\n",
    "    test_idx  = idx[:test_size]\n",
    "    train_idx = idx[test_size:]\n",
    "    \n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733bfb3",
   "metadata": {},
   "source": [
    "### Hàm `oversample_minority`\n",
    "\n",
    "Ở trong phần `Khám phá dữ liệu`, ta đã phát hiện cột đặc trưng `Attrition_Flag` bị **mất cân bằng dữ liệu** nên ta sẽ sử dụng kỹ thuật `random oversampling` để làm dữ liệu cân bằng trở lại. Ta sẽ nhân bản (duplicate) các mẫu dữ liệu thuộc lớp thiểu số (minority class) cho đến khi số lượng của nó tương đương với lớp đa số (majority class).\n",
    "\n",
    "### Bước 1: Tách dữ liệu (Separation)\n",
    "\n",
    "  * **Code:** `class0 = X[y == 0]` và `class1 = X[y == 1]`\n",
    "  * **Logic:** Chia tập dữ liệu ban đầu thành 2 nhóm riêng biệt dựa trên nhãn (label).\n",
    "\n",
    "### Bước 2: Tính hệ số nhân bản (Calculate Repeat Factor)\n",
    "\n",
    "  * **Code:** `repeat_factor = n1 // n0`\n",
    "  * **Logic:** Tính xem số lượng mẫu lớp đa số (n1) gấp bao nhiêu lần lớp thiểu số (n0).\n",
    "      * *Ví dụ:* Nếu Class 1 có 1000 mẫu, Class 0 có 100 mẫu $\\rightarrow$ Hệ số là 10.\n",
    "  * *Lưu ý:* Hàm này giả định `n1 > n0`. Nếu ngược lại, `repeat_factor` sẽ bằng 0 (gây lỗi mất dữ liệu).\n",
    "\n",
    "### Bước 3: Nhân bản (Duplication)\n",
    "\n",
    "  * **Code:** `np.repeat(class0, repeat_factor, axis=0)`\n",
    "  * **Logic:** Sao chép các dòng dữ liệu của Class 0 lên `repeat_factor` lần.\n",
    "      * Đây là cách tiếp cận \"Ngây thơ\" (Naive). Nó không tạo ra dữ liệu mới (như thuật toán SMOTE) mà chỉ đơn giản là copy-paste dữ liệu cũ.\n",
    "\n",
    "### Bước 4: Gộp và Xáo trộn (Concatenate & Shuffle)\n",
    "\n",
    "  * **Code:** `np.vstack`, `np.hstack` và `np.random.permutation`.\n",
    "  * **Logic:**\n",
    "    1.  Gộp Lớp 0 (đã nhân bản) và Lớp 1 (giữ nguyên) lại thành một tập dữ liệu mới (`X_balanced`).\n",
    "    2.  Tạo nhãn tương ứng (`y_balanced`).\n",
    "    3.  **Quan trọng:** Xáo trộn ngẫu nhiên vị trí các dòng (`idx`). Nếu không xáo trộn, dữ liệu sẽ bị sắp xếp theo thứ tự (toàn bộ lớp 0 rồi đến toàn bộ lớp 1), khiến mô hình học bị sai lệch (bias) trong quá trình Mini-batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa027957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_minority(X, y):\n",
    "    class0 = X[y == 0]\n",
    "    class1 = X[y == 1]\n",
    "\n",
    "    n0 = class0.shape[0]\n",
    "    n1 = class1.shape[0]\n",
    "\n",
    "    repeat_factor = n1 // n0\n",
    "\n",
    "    X0_over = np.repeat(class0, repeat_factor, axis=0)\n",
    "    y0_over = np.repeat(np.zeros(n0, dtype=int), repeat_factor)\n",
    "\n",
    "    X_balanced = np.vstack([X0_over, class1])\n",
    "    y_balanced = np.hstack([y0_over, np.ones(n1, dtype=int)])\n",
    "\n",
    "    idx = np.random.permutation(len(X_balanced))\n",
    "\n",
    "    return X_balanced[idx], y_balanced[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70ad68",
   "metadata": {},
   "source": [
    "## VI. Kỹ thuật K-Fold Cross-Validation \n",
    "\n",
    "Đây là một kỹ thuật đánh giá hiệu năng của mô hình máy học bằng cách chia dữ liệu thành $k$ phần nhỏ để đảm bảo mọi điểm dữ liệu đều được sử dụng cho cả việc huấn luyện và kiểm thử.\n",
    "\n",
    "Trong các phương pháp chia thông thường (như Train/Test split tỉ lệ 80/20), kết quả đánh giá có thể bị vấn đền chọn phải tập test dễ hay khó. Mục tiêu của K-Fold là:\n",
    "\n",
    "1.  **Đánh giá tin cậy hơn:** Giảm thiểu phương sai (variance) của kết quả đánh giá.\n",
    "2.  **Tận dụng dữ liệu:** Đảm bảo **100% dữ liệu** đều được dùng để test (ở các lần lặp khác nhau) và **(k-1)/k dữ liệu** được dùng để train.\n",
    "3.  **Chống Overfitting:** Giúp phát hiện xem mô hình có bị học vẹt trên một tập dữ liệu cụ thể nào không.\n",
    "\n",
    "### Các bước thực hiện\n",
    "\n",
    "1.  **Xáo trộn dữ liệu (`np.random.permutation`):**\n",
    "\n",
    "      * Trước khi chia, ta xáo trộn ngẫu nhiên danh sách chỉ số (index).\n",
    "      * *Lý do:* Để đảm bảo tính ngẫu nhiên, tránh trường hợp dữ liệu gốc đang được sắp xếp theo thứ tự (ví dụ: theo ngày tháng hoặc theo label), gây lệch phân phối giữa các fold.\n",
    "\n",
    "2.  **Chia kích thước (`m // k`):**\n",
    "\n",
    "      * Tính toán xem mỗi phần (fold) sẽ chứa bao nhiêu mẫu dữ liệu.\n",
    "\n",
    "3.  **Vòng lặp chia và xoay vòng (The Loop):**\n",
    "\n",
    "      * Thuật toán chạy $k$ lần. Tại mỗi vòng lặp $i$:\n",
    "          * **Xác định tập Test:** Chọn một đoạn chỉ số từ `start` đến `end` làm tập kiểm thử (Validation set).\n",
    "          * **Xác định tập Train:** Tất cả các chỉ số còn lại (trước đoạn `start` và sau đoạn `end`) được nối lại (`concatenate`) để làm tập huấn luyện.\n",
    "      * Kết quả là một danh sách chứa $k$ cặp (Train indices, Test indices).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2f8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_split(X, y, k=5):\n",
    "    m = len(X)\n",
    "    idx = np.random.permutation(m)\n",
    "    fold_size = m // k\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end   = start + fold_size\n",
    "        test_idx  = idx[start:end]\n",
    "        train_idx = np.concatenate([idx[:start], idx[end:]])\n",
    "        folds.append((train_idx, test_idx))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fbc6d9",
   "metadata": {},
   "source": [
    "## VII. Viết hàm chạy model `evaluate_models`\n",
    "\n",
    "Hàm này đóng vai trò là \"Tổng quản\", quản lý luồng dữ liệu từ đầu đến cuối để đảm bảo đánh giá công bằng.\n",
    "\n",
    "### Bước 1: Thiết lập môi trường\n",
    "\n",
    "  * Gọi hàm `k_fold_split` để chia dữ liệu thành $k$ phần.\n",
    "  * Khởi tạo từ điển `history` để lưu trữ kết quả (F1, Recall, Precision) của từng fold cho từng mô hình.\n",
    "\n",
    "### Bước 2: Vòng lặp kiểm định (The Validation Loop)\n",
    "\n",
    "Thuật toán chạy $k$ lần (ví dụ: 10 lần). Trong mỗi lần chạy (fold):\n",
    "\n",
    "1.  **Chia dữ liệu (Splitting):** Tách dữ liệu thành tập Train và tập Test dựa trên chỉ số (index) mà `k_fold_split` cung cấp.\n",
    "2.  **Xử lý mất cân bằng (Quan trọng):**\n",
    "      * Gọi hàm `oversample_minority` *tTrên tập train**.\n",
    "      * Nếu oversample trên toàn bộ dữ liệu trước khi chia, các bản sao của dữ liệu Train sẽ lọt vào tập Test. Điều này gọi là **Rò rỉ dữ liệu (Data Leakage)**, khiến kết quả kiểm tra cao ảo (mô hình học thuộc đề thi).\n",
    "      * Tập Test được giữ nguyên bản (nguyên thủy) để phản ánh đúng thực tế.\n",
    "3.  **Huấn luyện & Đánh giá (Train & Evaluate):**\n",
    "      * Lần lượt khởi tạo 3 mô hình (Logistic, KNN, GNB).\n",
    "      * **Fit:** Học từ dữ liệu Train đã được cân bằng (`X_train_os`).\n",
    "      * **Predict:** Dự đoán trên tập Test gốc (`X_test`).\n",
    "      * **Metric:** Tính điểm số và lưu vào danh sách lịch sử.\n",
    "\n",
    "### Bước 3: Tổng hợp kết quả (Aggregation)\n",
    "\n",
    "  * Sau khi chạy xong $k$ vòng lặp, ta có $k$ bảng kết quả cho mỗi mô hình.\n",
    "  * Dùng `np.mean(..., axis=0)` để tính trung bình cộng các chỉ số.\n",
    "  * Việc này giúp loại bỏ yếu tố ngẫu nhiên và đưa ra con số đáng tin cậy nhất về năng lực của mô hình.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9703c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, y, k=10):\n",
    "    folds = k_fold_split(X, y, k)\n",
    "    \n",
    "    history = {\n",
    "        'Logistic': [],\n",
    "        'KNN': [],\n",
    "        'GNB': []\n",
    "    }\n",
    "\n",
    "    print(f\"Starting {k}-Fold Cross-Validation...\")\n",
    "    print(\"=\"*60)\n",
    "    for fold in range(k):\n",
    "        print(f\"Processing Fold {fold+1}/{k}...\") \n",
    "        print(\"-\" * 60)\n",
    "        train_idx, test_idx = folds[fold]\n",
    "        \n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test,  y_test  = X[test_idx],  y[test_idx]\n",
    "\n",
    "        X_train_os, y_train_os = oversample_minority(X_train, y_train)\n",
    "\n",
    "        # --- 1. Logistic Regression ---\n",
    "        logi = LogisticRegression()\n",
    "        logi.fit(X_train_os, y_train_os, lr=0.01, epochs=800)\n",
    "        pred_logi = logi.predict(X_test)\n",
    "        metrics_logi = get_metrics(y_test, pred_logi) # Returns (f1, rec, pre)\n",
    "        f1, rec, pre = metrics_logi\n",
    "        print(\"Logistic => F1:\", f\"{f1:.2f}\", \"Recall:\", f\"{rec:.2f}\", \"Precision:\", f\"{pre:.2f}\")\n",
    "        history['Logistic'].append(metrics_logi)\n",
    "\n",
    "        # --- 2. KNN ---\n",
    "        knn = KNN()\n",
    "        knn.fit(X_train_os, y_train_os, k=5)\n",
    "        pred_knn = knn.predict(X_test)\n",
    "        metrics_knn = get_metrics(y_test, pred_knn)\n",
    "        f1, rec, pre = metrics_knn\n",
    "        print(\"KNN => F1:\", f\"{f1:.2f}\", \"Recall:\", f\"{rec:.2f}\", \"Precision:\", f\"{pre:.2f}\")\n",
    "        history['KNN'].append(metrics_knn)\n",
    "\n",
    "        # --- 3. Gaussian NB ---\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(X_train_os, y_train_os)\n",
    "        pred_nb = nb.predict(X_test)\n",
    "        metrics_nb = get_metrics(y_test, pred_nb)\n",
    "        f1, rec, pre = metrics_nb\n",
    "        print(\"GNB => F1:\", f\"{f1:.2f}\", \"Recall:\", f\"{rec:.2f}\", \"Precision:\", f\"{pre:.2f}\")\n",
    "        history['GNB'].append(metrics_nb)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary results across all folds\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    header = f\"{'Model':<15} | {'Avg F1':<12} | {'Avg Recall':<12} | {'Avg Precision':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    \n",
    "    for model_name, scores in history.items():\n",
    "        avg_scores = np.mean(scores, axis=0)\n",
    "        \n",
    "        f1_avg, rec_avg, pre_avg = avg_scores\n",
    "        \n",
    "        row = f\"{model_name:<15} | {f1_avg:<12.4f} | {rec_avg:<12.4f} | {pre_avg:<12.4f}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc244ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-Fold Cross-Validation...\n",
      "============================================================\n",
      "Processing Fold 1/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.86 Recall: 0.80 Precision: 0.92\n",
      "KNN => F1: 0.86 Recall: 0.81 Precision: 0.91\n",
      "GNB => F1: 0.83 Recall: 0.76 Precision: 0.91\n",
      "Processing Fold 2/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.87 Recall: 0.80 Precision: 0.95\n",
      "KNN => F1: 0.86 Recall: 0.80 Precision: 0.93\n",
      "GNB => F1: 0.84 Recall: 0.76 Precision: 0.93\n",
      "Processing Fold 3/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.87 Recall: 0.80 Precision: 0.96\n",
      "KNN => F1: 0.88 Recall: 0.82 Precision: 0.94\n",
      "GNB => F1: 0.85 Recall: 0.76 Precision: 0.95\n",
      "Processing Fold 4/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.88 Recall: 0.82 Precision: 0.94\n",
      "KNN => F1: 0.88 Recall: 0.84 Precision: 0.93\n",
      "GNB => F1: 0.85 Recall: 0.78 Precision: 0.93\n",
      "Processing Fold 5/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.87 Recall: 0.82 Precision: 0.94\n",
      "KNN => F1: 0.86 Recall: 0.80 Precision: 0.93\n",
      "GNB => F1: 0.84 Recall: 0.78 Precision: 0.93\n",
      "Processing Fold 6/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.87 Recall: 0.81 Precision: 0.94\n",
      "KNN => F1: 0.87 Recall: 0.82 Precision: 0.93\n",
      "GNB => F1: 0.85 Recall: 0.78 Precision: 0.94\n",
      "Processing Fold 7/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.87 Recall: 0.81 Precision: 0.94\n",
      "KNN => F1: 0.86 Recall: 0.81 Precision: 0.92\n",
      "GNB => F1: 0.83 Recall: 0.76 Precision: 0.93\n",
      "Processing Fold 8/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.86 Recall: 0.80 Precision: 0.92\n",
      "KNN => F1: 0.87 Recall: 0.81 Precision: 0.93\n",
      "GNB => F1: 0.82 Recall: 0.75 Precision: 0.92\n",
      "Processing Fold 9/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.85 Recall: 0.77 Precision: 0.95\n",
      "KNN => F1: 0.86 Recall: 0.80 Precision: 0.94\n",
      "GNB => F1: 0.82 Recall: 0.73 Precision: 0.95\n",
      "Processing Fold 10/10...\n",
      "------------------------------------------------------------\n",
      "Logistic => F1: 0.86 Recall: 0.80 Precision: 0.94\n",
      "KNN => F1: 0.87 Recall: 0.81 Precision: 0.94\n",
      "GNB => F1: 0.84 Recall: 0.75 Precision: 0.94\n",
      "\n",
      "============================================================\n",
      "Summary results across all folds\n",
      "============================================================\n",
      "Model           | Avg F1       | Avg Recall   | Avg Precision\n",
      "------------------------------------------------------------\n",
      "Logistic        | 0.8651       | 0.8024       | 0.9386      \n",
      "KNN             | 0.8666       | 0.8120       | 0.9294      \n",
      "GNB             | 0.8384       | 0.7616       | 0.9328      \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"../data/BankChurners_preprocessed.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "y = data[:, 0].astype(int)\n",
    "X = data[:, 1:]\n",
    "\n",
    "evaluate_models(X, y, k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
